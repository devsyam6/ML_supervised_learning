{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\n",
    "    'He ate sandwitches'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he ate sandwitch\n",
      "i love my countri\n",
      "jump\n",
      "jump\n",
      "jump\n",
      "eat\n",
      "ate\n",
      "eaten\n"
     ]
    }
   ],
   "source": [
    "\"\"\"STEMMING of words\"\"\"\n",
    "stemmer=PorterStemmer()\n",
    "print(stemmer.stem('He ate sandwitches'))\n",
    "print(stemmer.stem('i love my country'))\n",
    "print(stemmer.stem('jump'))\n",
    "print(stemmer.stem('jumping'))\n",
    "print(stemmer.stem('jumped'))\n",
    "print(stemmer.stem('eat'))\n",
    "print(stemmer.stem('ate'))\n",
    "print(stemmer.stem('eaten'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He ate sandwitches\n",
      "i love my country\n",
      "jump\n",
      "jumping\n",
      "jumped\n",
      "eat\n",
      "eat\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Lemmatizing of words\"\"\"\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('He ate sandwitches','n'))\n",
    "print(lemmatizer.lemmatize('i love my country','v'))\n",
    "print(lemmatizer.lemmatize('jump'))\n",
    "print(lemmatizer.lemmatize('jumping'))\n",
    "print(lemmatizer.lemmatize('jumped'))\n",
    "print(lemmatizer.lemmatize('eat','v'))\n",
    "print(lemmatizer.lemmatize('ate','v'))\n",
    "print(lemmatizer.lemmatize('eaten','v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cactus\n",
      "cacti\n",
      "goose\n",
      "gees\n",
      "good\n",
      "best\n",
      "better\n",
      "best\n",
      "run\n",
      "run\n",
      "runner\n",
      "ran\n",
      "run\n",
      "runner\n",
      "great\n",
      "great\n",
      "greater\n",
      "greatest\n",
      "index\n",
      "country\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('cacti'))\n",
    "print(stemmer.stem('cacti'))\n",
    "print(lemmatizer.lemmatize('geese'))\n",
    "print(stemmer.stem('geese'))\n",
    "print(lemmatizer.lemmatize('better',pos='a'))\n",
    "print(lemmatizer.lemmatize('best',pos='a'))\n",
    "print(stemmer.stem('better'))\n",
    "print(stemmer.stem('best'))\n",
    "print(lemmatizer.lemmatize('ran',pos='v'))\n",
    "print(lemmatizer.lemmatize('running',pos='v'))\n",
    "print(lemmatizer.lemmatize('runner',pos='v'))\n",
    "print(stemmer.stem('ran'))\n",
    "print(stemmer.stem('running'))\n",
    "print(stemmer.stem('runner'))\n",
    "print(lemmatizer.lemmatize('greater',pos='a'))\n",
    "print(lemmatizer.lemmatize('greatest',pos='a'))\n",
    "print(stemmer.stem('greater'))\n",
    "print(stemmer.stem('greatest'))\n",
    "print(lemmatizer.lemmatize('indices'))\n",
    "print(lemmatizer.lemmatize('countries'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"STEMMING..\"\"\"\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "corpus =['He ate every sandwitchs',\n",
    "         'every sandwitch was eaten by him'\n",
    "        ]\n",
    "corpus\n",
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He STeEMMED: He ate STeEMMED: ate every STeEMMED: everi sandwitchs STeEMMED: sandwitch every STeEMMED: everi sandwitch STeEMMED: sandwitch was STeEMMED: wa eaten STeEMMED: eaten by STeEMMED: by him STeEMMED: him "
     ]
    }
   ],
   "source": [
    "\"\"\"tokenizing \"\"\"\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\"\"\"METHOD 1\"\"\"\n",
    "for document in corpus:\n",
    "    for token in word_tokenize(document):\n",
    "        print(token,end=\" \")\n",
    "        print(\"STeEMMED:\",stemmer.stem(token),end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('H', 'NNP'), ('e', 'NN')]\n",
      "[('a', 'DT'), ('t', 'NN'), ('e', 'NN')]\n",
      "[('e', 'NN'), ('v', 'NN'), ('e', 'NN'), ('r', 'NN'), ('y', 'NN')]\n",
      "[('s', 'VB'), ('a', 'DT'), ('n', 'JJ'), ('d', 'NN'), ('w', 'NN'), ('i', 'NN'), ('t', 'VBP'), ('c', 'NN'), ('h', 'NN'), ('s', 'NN')]\n",
      "[('e', 'NN'), ('v', 'NN'), ('e', 'NN'), ('r', 'NN'), ('y', 'NN')]\n",
      "[('s', 'VB'), ('a', 'DT'), ('n', 'JJ'), ('d', 'NN'), ('w', 'NN'), ('i', 'NN'), ('t', 'VBP'), ('c', 'NN'), ('h', 'NN')]\n",
      "[('w', 'VB'), ('a', 'DT'), ('s', 'NN')]\n",
      "[('e', 'VB'), ('a', 'DT'), ('t', 'NN'), ('e', 'NN'), ('n', 'NN')]\n",
      "[('b', 'NN'), ('y', 'NN')]\n",
      "[('h', 'NN'), ('i', 'NN'), ('m', 'VBP')]\n",
      "STEMMED: ['He', 'ate', 'everi', 'sandwitch', 'everi', 'sandwitch', 'wa', 'eaten', 'by', 'him']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"METHOD 2\"\"\"\n",
    "stem_out=[]\n",
    "for document in corpus:\n",
    "    for token in word_tokenize(document):\n",
    "        stem_out.append(stemmer.stem(token))\n",
    "#         print(nltk.pos_tag(token))\n",
    "print('STEMMED:',stem_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed:  [['He', 'ate', 'everi', 'sandwitch'], ['everi', 'sandwitch', 'wa', 'eaten', 'by', 'him']]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"METHOD 3\"\"\"\n",
    " \n",
    "print('stemmed: ',[[stemmer.stem(token) for token in word_tokenize(document)]for document in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Word tokenzing and lemmatizing'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Word tokenzing and lemmatizing\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He PRP\n",
      "ate VB\n",
      "every DT\n",
      "sandwitchs NN\n",
      "every DT\n",
      "sandwitch NN\n",
      "was VBD\n",
      "eaten VBN\n",
      "by IN\n",
      "him PRP\n",
      "[['He', 'eat', 'every', 'sandwitchs'], ['every', 'sandwitch', 'be', 'eat', 'by', 'him']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "\n",
    "\n",
    "def lemma_identifier(token,tag):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    if tag[0].lower() in ['n','v']:\n",
    "        return lemmatizer.lemmatize(token,tag[0].lower())\n",
    "    return token\n",
    "\n",
    "#document splitted and word splitted in n and verb etc etc...\n",
    "tagged_corpus=[pos_tag(word_tokenize(document)) for document in corpus]\n",
    "\n",
    "'''\n",
    "METHOD 2 tokenize and tagging\n",
    "tagged_corpus=[]\n",
    "\n",
    "for document in corpus:\n",
    "    tagged_corpus.append(postag(word_tokenize(document)))\n",
    "'''\n",
    "   \n",
    "tagged_corpus\n",
    "\n",
    "for document in tagged_corpus:\n",
    "#     print(document)\n",
    "    for token,tag in document:\n",
    "        print(token,tag)\n",
    "#         print(\"lemmatized output:\",lemma_identifier(token,tag))\n",
    "        \n",
    "print([[lemma_identifier(token,tag) for token,tag in document] for document in tagged_corpus])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 1 3 2 1 1]]\n",
      "{'the': 4, 'dog': 2, 'ate': 1, 'sandwitch': 3, 'wizard': 6, 'transfigured': 5, 'and': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus=['the dog ate a sandwitch, the wizard transfigured a\\\n",
    "            sandwitch, and i ate a sandwitch']\n",
    "\n",
    "corpus\n",
    "\n",
    "vect=CountVectorizer()\n",
    "print(vect.fit_transform(corpus).todense())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
